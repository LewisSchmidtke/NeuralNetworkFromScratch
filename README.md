# NeuralNetworkFromScratch

## Overview
This project implements a simple neural network from scratch using Python, without relying on high-level machine learning frameworks. 
The goal is to understand the core mechanics of neural networks, including forward propagation, backpropagation, and optimization techniques.
### Update
- An Early Stopping mechanism has been built that prevents overfitting.
- First Visualization functions for training accuracy have been implemented/

## Features
- Fully connected neural network architecture for the Mnist dataset
- Forward and backward propagation implementation
- Basic activation functions: ReLU, Softmax
- Gradient descent optimization

## Planned Features
- Additional activation functions (Tanh, Sigmoid, Leaky ReLU, etc.)
- Early stopping implementation ✔️
- Visualization of loss curves and training progress ✔️
- Easily customizable layers and neurons 

## Contributing
If you'd like to contribute, feel free to open an issue or submit a pull request!
